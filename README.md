# ğŸ“˜ Azure Databricks Retail Analytics Lakehouse

## ğŸš€ Project Overview

This project demonstrates a **production-style Lakehouse pipeline** on **Azure Databricks** using an open **Online Retail dataset**.

It follows the **Bronze â†’ Silver â†’ Gold** architecture with **Delta Lake**, and ends with interactive **Databricks SQL dashboards** plus **Python visualizations**.

### Goals

* Ingest raw CSV data from **Azure Data Lake Storage Gen2**
* Store in Delta Lake Bronze, then clean in Silver
* Aggregate business insights into Gold tables
* Visualize with **Matplotlib** and **Databricks SQL dashboards**

---

## ğŸ—ï¸ Architecture

```
Raw CSV (ADLS Gen2)
        â”‚
        â–¼
 Bronze (Raw Delta)    â†’ stored as-is, schema-on-read
        â”‚
        â–¼
 Silver (Cleaned Delta) â†’ casted types, cleaned nulls, renamed cols
        â”‚
        â–¼
 Gold (Curated Delta)   â†’ Daily Revenue, Top Products, Aggregates
        â”‚
        â–¼
 Dashboards (Databricks SQL + Matplotlib)
```

---

## ğŸ“‚ Repository Structure

```
docs/
 â””â”€â”€ dashboard_screenshots/
     â”œâ”€â”€ dashboard_overview.png
     â”œâ”€â”€ daily_revenue_trend.png
     â”œâ”€â”€ top_countries.png
     â”œâ”€â”€ top_products.png
     â””â”€â”€ revenue_heatmap.png

notebooks/
 â”œâ”€â”€ 01_ingest_bronze_batch.ipynb
 â”œâ”€â”€ 01_ingest_bronze_autoloader.ipynb
 â”œâ”€â”€ 02_transform_silver_cleaning.ipynb
 â”œâ”€â”€ 03_gold_aggregations.ipynb
 â””â”€â”€ 04_analysis_visualization.ipynb

queries/
 â”œâ”€â”€ daily_revenue_trend.dbquery.ipynb
 â”œâ”€â”€ revenue_by_country_over_time.dbquery.ipynb
 â”œâ”€â”€ top_countries_total_revenue.dbquery.ipynb
 â”œâ”€â”€ top_products_quantity.dbquery.ipynb
 â””â”€â”€ top_countries_share.dbquery.ipynb

README.md
```

---

## ğŸ” Notebooks

1. **01 â€“ Bronze (Raw Ingestion)**
   * Ingests CSV files from ADLS Gen2
   * Saves to `retail_bronze` Delta table (batch + streaming/autoloader)

2. **02 â€“ Silver (Cleaning & Standardization)**
   * Casts columns (`Quantity`, `UnitPrice`, `InvoiceDate`)
   * Fixes decimal separators, timestamps
   * Drops nulls & renames columns
   * Inspects `_delta_log` for versioning and transactions

3. **03 â€“ Gold (Aggregations)**
   * Builds business-ready metrics:
     * `retail_gold_daily_sales` â†’ revenue per country/day
     * `retail_gold_top_products` â†’ top products by sales quantity

4. **04 â€“ Analysis & Visualization**
   * Matplotlib visualizations:
     * Daily revenue trend (with 7-day moving average)
     * Top 10 countries by revenue
     * Revenue distribution histogram
     * Top 10 products by quantity sold

---

## ğŸ“Š Databricks SQL Dashboard

Here are some visuals generated from Gold tables:

![Dashboard Overview](docs/dashboard_screenshots/dashboard_overview.png)
![Daily Revenue Trend](docs/dashboard_screenshots/daily_revenue_trend.png)
![Top 10 Products](docs/dashboard_screenshots/top_products.png)
![Countries Map](docs/dashboard_screenshots/revenue_heatmap.png)

You can find the SQL queries in the [queries/](queries/) folder.

### Visuals Included

* Daily Revenue Trend with 7-day Moving Average
* Top 10 Countries by Revenue
* Top Products by Quantity Sold
* Revenue Heatmap by Country/Day
* Monthly Revenue Growth

---

## ğŸ§¾ SQL Queries

All queries powering the dashboard are stored in [`/queries/`](queries/).

Example â€“ **Daily Revenue Trend**:

```sql
SELECT Date, SUM(Revenue) AS TotalRevenue
FROM retail_gold_daily_sales
GROUP BY Date
ORDER BY Date;
```

---

## ğŸ“Œ Technologies Used

* **Azure Databricks** (PySpark, Delta Lake, Unity Catalog, SQL)
* **Azure Data Lake Storage Gen2 (ADLS)**
* **Matplotlib** (Python visualization)
* **Databricks SQL Dashboards**

---

## ğŸ¯ Key Learnings

* Built an end-to-end **Lakehouse pipeline**
* Learned **Delta Lake internals** (`_delta_log`, tombstones, time travel)
* Designed **business-ready Gold tables**
* Created an **executive-style dashboard** combining SQL + Python visuals

---

## ğŸ“Œ How to Run

1. Upload notebooks from `/notebooks/` into your **Databricks workspace**
2. Configure ADLS external location for Delta tables
3. Run notebooks sequentially:
   * `01_ingest_bronze_batch.ipynb` or `01_ingest_bronze_autoloader.ipynb`
   * `02_transform_silver_cleaning.ipynb`
   * `03_gold_aggregations.ipynb`
   * `04_analysis_visualization.ipynb`
4. In Databricks SQL â†’ import queries from `/queries/` and build the dashboard

---

âœ¨ This project shows how to build a **real-world analytics solution** on Azure Databricks.
It's end-to-end: **Ingestion â†’ Transformation â†’ Aggregation â†’ Visualization**.